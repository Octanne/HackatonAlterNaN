
# ------------------------------------------------------------------------------
Execution of code_aster

# ------------------------------------------------------------------------------
Prepare environment in /tmp/run_aster_iwfw1npd/proc.0

# ------------------------------------------------------------------------------
Command file #1 / 1

Content of the file to execute:
# coding=utf-8
#!/usr/bin/python

import os
from statistics import mean
from datetime import datetime
from resource import RUSAGE_SELF, getrusage

from code_aster.Commands import *
from code_aster import CA
from code_aster.Utilities import petscInitialize

CA.init()

params = {}
params["refinements"] = int(os.environ.get("REFINE", 1))
params["parallel"] = os.environ.get("USE_LEGACY", "HPC")
params["solver"] = os.environ.get("SOLVER", "PETSC")

# General parameters
comm = CA.MPI.ASTER_COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

nbHexa = 8 ** params["refinements"]


def memory_peak(mess=None):
    """Return memory peak in MB"""
    return int(getrusage(RUSAGE_SELF).ru_maxrss / 1024)


class ChronoCtxMgGen:
    stats = {}

    def __init__(self, what):
        self._what = what

    def __enter__(self):
        self.start = datetime.now()

    def __exit__(self, exctype, exc, tb):
        self.stop = datetime.now()
        delta = self.stop - self.start
        mem = memory_peak(self._what)
        self.stats[self._what] = [delta.total_seconds(), mem]


class ChronoCtxMg(ChronoCtxMgGen):
    pass
    # def __init__(self, what):
    #     ChronoCtxMgGen.__init__(self, what)


def write_stats(nume_ddl):
    if rank == 0:
        print("TITLE: TEST PERF CUBE")
        print()
        print("NB PROC")
        print(size)
        print()
        print(
            "COMMAND, TIME MIN (s), TIME MAX (s), TIME MEAN (s), MEM MIN (Mo), MEM MAX (Mo), MEM MEAN (Mo)"
        )

    for key, values in stats.items():
        time = comm.gather(values[0], root=0)
        mem = comm.gather(values[1], root=0)
        if rank == 0:
            print(
                key
                + ", "
                + str(min(time))
                + ", "
                + str(max(time))
                + ", "
                + str(mean(time))
                + ", "
                + str(min(mem))
                + ", "
                + str(max(mem))
                + ", "
                + str(mean(mem))
            )

    mesh = nume_ddl.getMesh()
    nodes = len(mesh.getInnerNodes())
    nodes = comm.allreduce(nodes, CA.MPI.SUM)

    if rank == 0:
        print()
        print("NB CELLS, NB NODES, NB DOFS")
        print(str(nbHexa) + ", " + str(nodes) + ", " + str(nume_ddl.getNumberOfDofs()))


def print_markdown_table(data, refine, nbcells, nbnodes, nbdofs):
    """Print a table of the mean time as a Markdown table."""

    def show(*args, **kwargs):
        if rank == 0:
            print(*args, **kwargs)

    fmti = "| {0:<16s} | {1:11,d} |"
    fmtt = "| {0:<16s} | {1:11.2f} |"
    separ = "| :--------------- | ----------: |"
    show(fmti.format("Refinement", refine))
    show(separ)
    show(fmti.format("Number of cells", nbcells).replace(",", " "))
    show(fmti.format("Number of nodes", nbnodes).replace(",", " "))
    show(fmti.format("Number of DOFs", nbdofs).replace(",", " "))
    show(fmti.format("Number of procs", size).replace(",", " "))
    show(fmti.format("Nb of DOFs/proc", nbdofs // size).replace(",", " "))
    for key, values in data.items():
        times = comm.gather(values[0], root=0)
        # mem = comm.gather(values[1], root=0)
        if rank == 0:
            show(fmtt.format(key, mean(times)))


# petscInitialize('-ksp_monitor_true_residual -stats' )
petscInitialize("-ksp_monitor_true_residual -log_view")

with ChronoCtxMg("Total"):
    with ChronoCtxMg("Build mesh"):
        if params["parallel"] == "HPC":
            mesh = CA.ParallelMesh.buildCube(refine=params["refinements"])
        else:
            mesh = CA.Mesh.buildCube(refine=params["refinements"])

    with ChronoCtxMg("Model"):
        model = AFFE_MODELE(
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                PHENOMENE="MECANIQUE",
                MODELISATION="3D",
            ),
        )

    with ChronoCtxMg("Material"):
        steel = DEFI_MATERIAU(
            ELAS=_F(
                E=200000.0,
                NU=0.3,
            ),
            ECRO_LINE=_F(
                D_SIGM_EPSI=2000.0,
                SY=200.0,
            ),
        )

        mater = AFFE_MATERIAU(
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                MATER=steel,
            ),
        )

    with ChronoCtxMg("Boundary conditions"):
        block = AFFE_CHAR_CINE(
            MODELE=model,
            MECA_IMPO=(
                _F(
                    GROUP_MA="LEFT",
                    DX=0,
                    DY=0.0,
                    DZ=0.0,
                ),
            ),
        )

        imposed_displ = AFFE_CHAR_CINE(
            MODELE=model,
            MECA_IMPO=(
                _F(
                    GROUP_MA="RIGHT",
                    DY=0.001,
                    DZ=0.001,
                ),
            ),
        )

    with ChronoCtxMg("Create matrix"):
        stiff_elem = CALC_MATR_ELEM(
            MODELE=model,
            OPTION="RIGI_MECA",
            CHAM_MATER=mater,
        )

    with ChronoCtxMg("Numbering"):
        dofNum = NUME_DDL(
            MATR_RIGI=stiff_elem,
        )

    with ChronoCtxMg("Assembly"):
        stiffness = ASSE_MATRICE(
            MATR_ELEM=stiff_elem,
            NUME_DDL=dofNum,
            CHAR_CINE=(block, imposed_displ),
        )

    with ChronoCtxMg("Build RHS"):
        rhs = CREA_CHAMP(
            TYPE_CHAM="NOEU_DEPL_R",
            OPERATION="AFFE",
            MAILLAGE=mesh,
            AFFE=_F(
                TOUT="OUI",
                NOM_CMP=(
                    "DX",
                    "DY",
                    "DZ",
                ),
                VALE=(
                    0.0,
                    0.0,
                    0.0,
                ),
            ),
        )

        load_vector = CALC_CHAR_CINE(NUME_DDL=dofNum, CHAR_CINE=(block, imposed_displ))

    if params["solver"] == "PETSC":
        solver = CA.PetscSolver(RENUM="SANS", PRE_COND="GAMG")
    elif params["solver"] == "MUMPS":
        solver = CA.MumpsSolver(
            MATR_DISTRIBUEE="OUI",
            RENUM="PARMETIS",
            ACCELERATION="FR+",
            POSTTRAITEMENTS="MINI",
        )

    with ChronoCtxMg("Factorize"):
        solver.factorize(stiffness)

    with ChronoCtxMg("Solve"):
        resu = solver.solve(rhs, load_vector)

# write_stats(dofNum)
nbNodes = len(mesh.getInnerNodes())
if params["parallel"] == "HPC":
    nbNodes = comm.allreduce(nbNodes, CA.MPI.SUM)
nbDOFs = dofNum.getNumberOfDOFs()
print_markdown_table(ChronoCtxMg.stats, params["refinements"], nbHexa, nbNodes, nbDOFs)

CA.close()



# ------------------------------------------------------------------------------
Command line #1:
    ulimit -c unlimited ; ulimit -t 108000 ; ( /opt/venv/bin/python3 -m mpi4py /home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/Cube_perf.py --last --tpmax 86400 ; echo $? > _exit_code_ ) 2>&1 | tee -a fort.6
setting '--memory' value to 3686.40 MB (keyword RESERVE_MEMOIRE)
checking MPI initialization...
using COMM_WORLD.
MPI is initialized.
Ouverture en écriture du fichier ./vola.1

<INFO> Démarrage de l'exécution.

                       -- CODE_ASTER -- VERSION : DÉVELOPPEMENT (unstable) --                       
                               Version 17.2.4 modifiée le 20/01/2025                                
                               révision f855b56619c7 - branche 'main'                               
                                   Copyright EDF R&D 1991 - 2025                                    
                                                                                                    
                              Exécution du : Fri Jan 24 13:34:41 2025                               
                                  Nom de la machine : fe732af82b6a                                  
                                        Architecture : 64bit                                        
                                    Type de processeur : aarch64                                    
        Système d'exploitation : Linux-5.10.226-214.880.amzn2.aarch64-aarch64-with-glibc2.40        
                                  Langue des messages : en (UTF-8)                                  
                                     Version de Python : 3.11.2                                     
                                     Version de NumPy : 1.24.2                                      
                                      Parallélisme MPI : actif                                      
                                   Rang du processeur courant : 0                                   
                               Nombre de processeurs MPI utilisés : 1                               
                                    Parallélisme OpenMP : actif                                     
                              Nombre de processus OpenMP utilisés : 1                               
                               Version de la librairie HDF5 : 1.10.9                                
                                Version de la librairie MED : 4.1.1                                 
                               Version de la librairie MFront : 4.2.0                               
                               Version de la librairie MUMPS : 5.6.2                                
                              Version de la librairie PETSc : 3.20.5p0                              
                               Version de la librairie SCOTCH : 7.0.4                               

starting the execution...
Valeur initiale du temps CPU maximum =   86400 secondes
  Valeur du temps CPU maximum passé aux commandes =   77760 secondes
  Réserve CPU prévue = 8640 secondes

Ouverture en écriture du fichier ./glob.1

Ouverture en écriture du fichier ./vola.1

Ouverture en lecture du fichier /opt/aster/install/mpi/lib/aster/elem.1

Nom de la base                          :  ELEMBASE
     Créée avec la version                   :  17.02.04
     Nombre d'enregistrements utilisés       :  45
     Nombre d'enregistrements maximum        :  512
     Nombre d'enregistrements par fichier    :  512
     Longueur d'enregistrement (octets)      :  819200
     Nombre d'identificateurs utilisés       :  123
     Taille maximum du répertoire            :  300
     Pourcentage d'utilisation du répertoire :  41 %

Ouverture en lecture du fichier /opt/aster/install/mpi/lib/aster/elem.1

Nom de la base                          :  ELEMBASE
     Nombre d'enregistrements utilisés       :  45
     Nombre d'enregistrements maximum        :  512
     Nombre d'enregistrements par fichier    :  512
     Longueur d'enregistrement (octets)      :  819200
     Nombre total d'accès en lecture         :  63
     Volume des accès en lecture             :         49.22 Mo.
     Nombre total d'accès en écriture        :  0
     Volume des accès en écriture            :          0.00 Mo.
     Nombre d'identificateurs utilisés       :  123
     Taille maximum du répertoire            :  300
     Pourcentage d'utilisation du répertoire :  41 %

Relecture des catalogues des éléments faite.

Fin de lecture (durée  0.018336  s.) 

                      Mémoire limite pour l'allocation dynamique : 4198.49 Mo                       
                         ajouté à l'initialisation du processus : 618.39 Mo                         
                               Limite cible du processus : 4816.88 Mo                               
                         Taille limite des fichiers d'échange : 2048.00 Go                          
# Mémoire (Mo) :   618.39 /   609.52 /   209.22 /   185.03 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0001   user+syst:        0.03s (syst:        0.10s, elaps:        0.13s)
# ----------------------------------------------------------------------------------------------
PETSc initialized...
Nom MED du maillage : PARALLEPIPED


------------ MAILLAGE 00000001 - IMPRESSIONS NIVEAU  1 ------------


NOMBRE DE NOEUDS                      274625

NOMBRE DE MAILLES                     287488
                              SEG2                  768
                              QUAD4               24576
                              HEXA8              262144

NOMBRE DE GROUPES DE NOEUDS                8

NOMBRE DE GROUPES DE MAILLES              19

--------------------------------------------------------------------------------


.. _stg1_txt190
# ----------------------------------------------------------------------------------------------
# Commande #0002 de /opt/aster/install/mpi/lib/aster/code_aster/Helpers/LogicalUnit.py, ligne 190
DEFI_FICHIER(ACCES='NEW',
             ACTION='ASSOCIER',
             FICHIER='/tmp/buildCube0hcuha14/buildCube.med',
             TYPE='BINARY',
             UNITE=99)

Deleting '/tmp/buildCube0hcuha14/buildCube.med': No such file or directory
# Mémoire (Mo) :  1106.18 /   775.80 /   249.04 /   213.86 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0002   user+syst:        0.00s (syst:        0.00s, elaps:        0.00s)
# ----------------------------------------------------------------------------------------------
Création du fichier au format MED 3.3.1.


.. _stg1_txt190
# ----------------------------------------------------------------------------------------------
# Commande #0003 de /opt/aster/install/mpi/lib/aster/code_aster/Helpers/LogicalUnit.py, ligne 190
DEFI_FICHIER(ACTION='LIBERER',
             UNITE=99)

# Mémoire (Mo) :  1106.18 /   775.92 /   282.08 /   250.98 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0003   user+syst:        0.00s (syst:        0.01s, elaps:        0.01s)
# ----------------------------------------------------------------------------------------------
<INFO> Activation du mode parallélisme distribué.

Nom MED du maillage : 00000001


--------------------------------------------------------------------------------


--------------------------------------------------------------------------------


.. _stg1_txt282
# ----------------------------------------------------------------------------------------------
# Commande #0004 de /opt/aster/install/mpi/lib/aster/code_aster/ObjectsExt/parallelmesh_ext.py,
ligne 282
CREA_MAILLAGE(INFO=1,
              MAILLAGE='<00000002>',
              RAFFINEMENT=_F(NIVEAU=0,
                             TOUT='OUI'))


------------ MAILLAGE 00000004 - IMPRESSIONS NIVEAU  1 ------------

ASTER 17.02.04 CONCEPT 00000004 CALCULE LE 24/01/2025 A 13:35:01 DE TYPE        
MAILLAGE_P                                                                      

NOMBRE DE NOEUDS                      274625

NOMBRE DE MAILLES                     287488
                              SEG2                  768
                              QUAD4               24576
                              HEXA8              262144

NOMBRE DE GROUPES DE NOEUDS                8

NOMBRE DE GROUPES DE MAILLES              19

--------------------------------------------------------------------------------

#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0004 (CREA_MAILLAGE): '<00000004>' de type <ParallelMesh>
# Mémoire (Mo) :  2566.60 /  1155.15 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0004   user+syst:        3.72s (syst:        0.42s, elaps:        4.15s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt131
# ----------------------------------------------------------------------------------------------
# Commande #0005 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 131
model = AFFE_MODELE(AFFE=_F(MODELISATION='3D',
                            PHENOMENE='MECANIQUE',
                            TOUT='OUI'),
                    DISTRIBUTION=_F(METHODE='CENTRALISE'),
                    INFO=1,
                    MAILLAGE='<00000004>',
                    VERI_JACOBIEN='OUI',
                    VERI_NORM_IFS='OUI',
                    VERI_PLAN='OUI')

Sur les 287488 mailles du maillage 00000004, on a demandé l'affectation de 287488, on a pu en
affecter 287488.
Modélisation     Formulation      Type maille  Élément fini     Nombre
_                _                SEG2         MECA_ARETE2      768
_                _                QUAD4        MECA_FACE4       24576
3D               _                HEXA8        MECA_HEXA8       262144
#2      Calculs elementaires et assemblages               CPU (USER+SYST/SYST/ELAPS):      0.19      0.00      0.19
#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0005 (AFFE_MODELE): model ('<00000005>') de type <Model>
# Mémoire (Mo) :  2566.60 /  1145.34 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0005   user+syst:        1.25s (syst:        0.00s, elaps:        1.26s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt141
# ----------------------------------------------------------------------------------------------
# Commande #0006 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 141
steel = DEFI_MATERIAU(ECRO_LINE=_F(D_SIGM_EPSI=2000.0,
                                   SY=200.0),
                      ELAS=_F(B_ENDOGE=0.0,
                              COEF_AMOR=1.0,
                              E=200000.0,
                              K_DESSIC=0.0,
                              NU=0.3),
                      INFO=1)

# Résultat commande #0006 (DEFI_MATERIAU): steel ('<00000006>') de type <Material>
# Mémoire (Mo) :  2566.60 /  1145.34 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0006   user+syst:        0.03s (syst:        0.00s, elaps:        0.02s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt152
# ----------------------------------------------------------------------------------------------
# Commande #0007 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 152
mater = AFFE_MATERIAU(AFFE=_F(MATER=steel,
                              TOUT='OUI'),
                      INFO=1,
                      MAILLAGE='<00000004>')

# Résultat commande #0007 (AFFE_MATERIAU): mater ('<00000007>') de type <MaterialField>
# Mémoire (Mo) :  2566.60 /  1145.34 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0007   user+syst:        0.01s (syst:        0.00s, elaps:        0.02s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt161
# ----------------------------------------------------------------------------------------------
# Commande #0008 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 161
block = AFFE_CHAR_CINE(INFO=1,
                       MECA_IMPO=_F(DX=0,
                                    DY=0.0,
                                    DZ=0.0,
                                    GROUP_MA='LEFT'),
                       MODELE=model,
                       SYNTAXE='NON')

# Résultat commande #0008 (AFFE_CHAR_CINE): block ('<00000008>') de type <MechanicalDirichletBC>
# Mémoire (Mo) :  2566.60 /  1145.34 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0008   user+syst:        0.17s (syst:        0.00s, elaps:        0.16s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt173
# ----------------------------------------------------------------------------------------------
# Commande #0009 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 173
imposed_displ = AFFE_CHAR_CINE(INFO=1,
                               MECA_IMPO=_F(DY=0.001,
                                            DZ=0.001,
                                            GROUP_MA='RIGHT'),
                               MODELE=model,
                               SYNTAXE='NON')

# Résultat commande #0009 (AFFE_CHAR_CINE): imposed_displ ('<00000009>') de type
<MechanicalDirichletBC>
# Mémoire (Mo) :  2566.60 /  1145.34 /   366.25 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0009   user+syst:        0.16s (syst:        0.00s, elaps:        0.17s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt185
# ----------------------------------------------------------------------------------------------
# Commande #0010 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 185
stiff_elem = CALC_MATR_ELEM(CALC_ELEM_MODELE='OUI',
                            CHAM_MATER=mater,
                            INST=0.0,
                            MODELE=model,
                            MODE_FOURIER=0,
                            OPTION='RIGI_MECA')

# Résultat commande #0010 (CALC_MATR_ELEM): stiff_elem ('<0000000b>') de type
<ElementaryMatrixDisplacementReal>
# Mémoire (Mo) :  2566.60 /  1460.43 /   880.92 /   324.46 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0010   user+syst:        2.98s (syst:        0.04s, elaps:        3.02s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt192
# ----------------------------------------------------------------------------------------------
# Commande #0011 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 192
dofNum = NUME_DDL(INFO=1,
                  MATR_RIGI=stiff_elem)

Le système linéaire à résoudre a 823875 degrés de liberté:
   - 823875 sont des degrés de liberté physiques
     (ils sont portés par 274625 noeuds du maillage)
   - 0 sont les couples de paramètres de Lagrange associés
     aux 0 relations linéaires dualisées.
La matrice est de taille 823875 équations.
  Elle contient 32762694 termes non nuls si elle est symétrique et 64701513 termes non nuls si elle
n'est pas symétrique.
  Soit un taux de remplissage de   0.010 %.
# Résultat commande #0011 (NUME_DDL): dofNum ('<00000011>') de type <ParallelDOFNumbering>
# Mémoire (Mo) :  2566.60 /  1648.29 /  1724.47 /  1037.52 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0011   user+syst:        1.48s (syst:        0.78s, elaps:        2.26s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt197
# ----------------------------------------------------------------------------------------------
# Commande #0012 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 197
stiffness = ASSE_MATRICE(CHAR_CINE=(block, imposed_displ),
                         INFO=1,
                         MATR_ELEM=stiff_elem,
                         NUME_DDL=dofNum)

# Résultat commande #0012 (ASSE_MATRICE): stiffness ('<00000013>') de type
<AssemblyMatrixDisplacementReal>
# Mémoire (Mo) :  2566.60 /  1904.54 /  1724.47 /  1037.52 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0012   user+syst:        1.21s (syst:        0.06s, elaps:        1.28s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt204
# ----------------------------------------------------------------------------------------------
# Commande #0013 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 204
rhs = CREA_CHAMP(AFFE=_F(NOM_CMP=('DX', 'DY', 'DZ'),
                         TOUT='OUI',
                         VALE=(0.0, 0.0, 0.0)),
                 INFO=1,
                 MAILLAGE='<00000004>',
                 OPERATION='AFFE',
                 TYPE_CHAM='NOEU_DEPL_R')

#4      Communications MPI                                CPU (USER+SYST/SYST/ELAPS):      0.00      0.00      0.00
# Résultat commande #0013 (CREA_CHAMP): rhs ('<00000015>') de type <FieldOnNodesReal>
# Mémoire (Mo) :  2566.60 /  1954.84 /  1724.47 /  1037.52 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0013   user+syst:        0.04s (syst:        0.01s, elaps:        0.05s)
# ----------------------------------------------------------------------------------------------

.. _stg1_txt223
# ----------------------------------------------------------------------------------------------
# Commande #0014 de
/home/aster/work/Benchmark-CodeAster/opensource-installation-development-main/benchmarks/Cube_files/
Cube_perf.py, ligne 223
load_vector = CALC_CHAR_CINE(CHAR_CINE=(block, imposed_displ),
                             INFO=1,
                             INST=0.0,
                             NUME_DDL=dofNum)

# Résultat commande #0014 (CALC_CHAR_CINE): load_vector ('<00000017>') de type <FieldOnNodesReal>
# Mémoire (Mo) :  2566.60 /  1961.13 /  1724.47 /  1037.52 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0014   user+syst:        0.03s (syst:        0.01s, elaps:        0.04s)
# ----------------------------------------------------------------------------------------------
  0 KSP unpreconditioned resid norm 2.784557239768e+02 true resid norm 2.784557239768e+02 ||r(i)||/||b|| 1.000000000000e+00
  1 KSP unpreconditioned resid norm 4.008438431540e+01 true resid norm 4.008438431540e+01 ||r(i)||/||b|| 1.439524522712e-01
  2 KSP unpreconditioned resid norm 5.877137724255e+00 true resid norm 5.877137724255e+00 ||r(i)||/||b|| 2.110618392152e-02
  3 KSP unpreconditioned resid norm 2.017281199044e+00 true resid norm 2.017281199044e+00 ||r(i)||/||b|| 7.244531267788e-03
  4 KSP unpreconditioned resid norm 8.902535310549e-01 true resid norm 8.902535310548e-01 ||r(i)||/||b|| 3.197109825363e-03
  5 KSP unpreconditioned resid norm 4.160076693929e-01 true resid norm 4.160076693929e-01 ||r(i)||/||b|| 1.493981389399e-03
  6 KSP unpreconditioned resid norm 1.949452678512e-01 true resid norm 1.949452678512e-01 ||r(i)||/||b|| 7.000943096702e-04
  7 KSP unpreconditioned resid norm 9.296903880895e-02 true resid norm 9.296903880894e-02 ||r(i)||/||b|| 3.338736854865e-04
  8 KSP unpreconditioned resid norm 4.375052663087e-02 true resid norm 4.375052663087e-02 ||r(i)||/||b|| 1.571184316345e-04
  9 KSP unpreconditioned resid norm 2.035746447264e-02 true resid norm 2.035746447264e-02 ||r(i)||/||b|| 7.310844317330e-05
 10 KSP unpreconditioned resid norm 9.631135338152e-03 true resid norm 9.631135338151e-03 ||r(i)||/||b|| 3.458767232579e-05
 11 KSP unpreconditioned resid norm 4.572313873859e-03 true resid norm 4.572313873859e-03 ||r(i)||/||b|| 1.642025456887e-05
 12 KSP unpreconditioned resid norm 2.119706969561e-03 true resid norm 2.119706969565e-03 ||r(i)||/||b|| 7.612366301157e-06
 13 KSP unpreconditioned resid norm 1.007434198883e-03 true resid norm 1.007434198881e-03 ||r(i)||/||b|| 3.617933165434e-06
 14 KSP unpreconditioned resid norm 4.888194074569e-04 true resid norm 4.888194074564e-04 ||r(i)||/||b|| 1.755465466737e-06
 15 KSP unpreconditioned resid norm 2.498434082016e-04 true resid norm 2.498434082026e-04 ||r(i)||/||b|| 8.972464441901e-07
| Refinement       |           6 |
| :--------------- | ----------: |
| Number of cells  |     262 144 |
| Number of nodes  |     274 625 |
| Number of DOFs   |     823 875 |
| Number of procs  |           1 |
| Nb of DOFs/proc  |     823 875 |
| Build mesh       |       20.80 |
| Model            |        1.25 |
| Material         |        0.04 |
| Boundary conditions |        0.33 |
| Create matrix    |        3.03 |
| Numbering        |        2.26 |
| Assembly         |        1.28 |
| Build RHS        |        0.09 |
| Factorize        |       17.87 |
| Solve            |        5.99 |
| Total            |       52.94 |

.. _stg1_txt72
# ----------------------------------------------------------------------------------------------
# Commande #0015 de /opt/aster/install/mpi/lib/aster/code_aster/CodeCommands/fin.py, ligne 72
FIN(INFO_RESU='NON',
    RETASSAGE='NON')

No database in results, objects not saved on processor #0
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

petsc_aster on a  named fe732af82b6a with 1 processor, by Unknown Fri Jan 24 13:35:34 2025
Using 1 OpenMP threads
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           5.296e+01     1.000   5.296e+01
Objects:              0.000e+00     0.000   0.000e+00
Flops:                2.442e+10     1.000   2.442e+10  2.442e+10
Flops/sec:            4.611e+08     1.000   4.611e+08  4.611e+08
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 5.2962e+01 100.0%  2.4423e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided         96 1.0 1.3012e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF        46 1.0 1.6819e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMult              312 1.0 5.5541e+00 1.0 1.42e+10 1.0 0.0e+00 0.0e+00 0.0e+00 10 58  0  0  0  10 58  0  0  0  2554
MatMultAdd            60 1.0 2.3255e-01 1.0 6.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2708
MatMultTranspose      60 1.0 2.5254e-01 1.0 6.30e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  2494
MatSolve              15 1.0 3.6684e-05 1.0 9.90e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    27
MatLUFactorSym         1 1.0 8.6340e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         1 1.0 3.1930e-06 1.0 1.29e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    40
MatConvert             1 1.0 2.1016e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatScale               8 1.0 5.0618e-02 1.0 4.20e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   829
MatResidual           60 1.0 8.3900e-01 1.0 2.17e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  9  0  0  0   2  9  0  0  0  2587
MatAssemblyBegin      74 1.0 5.8405e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        74 1.0 2.1262e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 1.5180e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         1 1.0 2.5629e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCoarsen             4 1.0 2.5014e-01 1.0 1.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    76
MatZeroEntries         5 1.0 1.3201e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAXPY                8 1.0 4.1711e-01 1.0 1.25e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    30
MatTranspose          18 1.0 2.0203e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultSym         15 1.0 2.0502e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
MatMatMultNum         15 1.0 1.7915e+00 1.0 3.64e+09 1.0 0.0e+00 0.0e+00 0.0e+00  3 15  0  0  0   3 15  0  0  0  2034
MatPtAPSymbolic        5 1.0 4.8489e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  9  0  0  0  0   9  0  0  0  0     0
MatPtAPNumeric         5 1.0 3.1857e+00 1.0 6.69e+09 1.0 0.0e+00 0.0e+00 0.0e+00  6 27  0  0  0   6 27  0  0  0  2101
MatGetBrAoCol          1 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot               58 1.0 5.6863e-02 1.0 3.11e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  5474
VecNorm               81 1.0 9.8773e-03 1.0 7.97e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  8071
VecScale              63 1.0 5.4422e-03 1.0 2.50e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4599
VecCopy              201 1.0 1.3662e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet               212 1.0 3.5484e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY               20 1.0 6.3340e-03 1.0 2.81e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4431
VecAYPX              377 1.0 6.9371e-02 1.0 1.16e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1676
VecAXPBYCZ           120 1.0 2.9015e-02 1.0 1.28e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  4407
VecMAXPY              78 1.0 9.9478e-02 1.0 5.51e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  5537
VecAssemblyBegin       1 1.0 1.3595e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd         1 1.0 2.2310e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult     284 1.0 5.1318e-02 1.0 6.05e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1179
VecScatterBegin      432 1.0 6.6661e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterEnd        432 1.0 6.8232e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          47 1.0 5.7946e-03 1.0 3.55e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  6134
SFSetGraph            50 1.0 1.2028e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               50 1.0 2.3077e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastBegin          15 1.0 2.6009e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFBcastEnd            15 1.0 1.0406e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceBegin          8 1.0 2.7276e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFReduceEnd            8 1.0 7.4970e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFPack               455 1.0 7.6743e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack             455 1.0 9.7110e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp              11 1.0 2.6942e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.9031e+00 1.0 1.49e+10 1.0 0.0e+00 0.0e+00 0.0e+00 11 61  0  0  0  11 61  0  0  0  2521
KSPGMRESOrthog        55 1.0 1.0600e-01 1.0 5.83e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  5500
PCSetUp_GAMG+          1 1.0 1.5680e+01 1.0 9.36e+09 1.0 0.0e+00 0.0e+00 0.0e+00 30 38  0  0  0  30 38  0  0  0   597
 PCGAMGCreateG         4 1.0 4.7568e+00 1.0 7.23e+07 1.0 0.0e+00 0.0e+00 0.0e+00  9  0  0  0  0   9  0  0  0  0    15
 GAMG Coarsen          4 1.0 2.7627e-01 1.0 1.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0    68
  GAMG MIS/Agg         4 1.0 2.5016e-01 1.0 1.89e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    76
 PCGAMGProl            4 1.0 1.5036e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  GAMG Prol-col        4 1.0 5.4161e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
  GAMG Prol-lift       4 1.0 1.3912e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
 PCGAMGOptProl         4 1.0 2.5732e+00 1.0 2.60e+09 1.0 0.0e+00 0.0e+00 0.0e+00  5 11  0  0  0   5 11  0  0  0  1009
  GAMG smooth          4 1.0 1.8772e+00 1.0 9.15e+08 1.0 0.0e+00 0.0e+00 0.0e+00  4  4  0  0  0   4  4  0  0  0   488
 PCGAMGCreateL         4 1.0 7.9016e+00 1.0 6.67e+09 1.0 0.0e+00 0.0e+00 0.0e+00 15 27  0  0  0  15 27  0  0  0   845
  GAMG PtAP            4 1.0 7.9016e+00 1.0 6.67e+09 1.0 0.0e+00 0.0e+00 0.0e+00 15 27  0  0  0  15 27  0  0  0   845
PCGAMG Gal l00         1 1.0 6.4786e+00 1.0 5.34e+09 1.0 0.0e+00 0.0e+00 0.0e+00 12 22  0  0  0  12 22  0  0  0   824
PCGAMG Opt l00         1 1.0 1.4450e+00 1.0 7.76e+08 1.0 0.0e+00 0.0e+00 0.0e+00  3  3  0  0  0   3  3  0  0  0   537
PCGAMG Gal l01         1 1.0 1.3167e+00 1.0 1.24e+09 1.0 0.0e+00 0.0e+00 0.0e+00  2  5  0  0  0   2  5  0  0  0   940
PCGAMG Opt l01         1 1.0 1.4241e-01 1.0 7.83e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   549
PCGAMG Gal l02         1 1.0 1.0611e-01 1.0 1.00e+08 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   944
PCGAMG Opt l02         1 1.0 2.0983e-02 1.0 1.34e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   640
PCGAMG Gal l03         1 1.0 2.2993e-04 1.0 6.74e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   293
PCGAMG Opt l03         1 1.0 1.6844e-04 1.0 6.22e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   369
PCSetUp                2 1.0 1.5680e+01 1.0 9.36e+09 1.0 0.0e+00 0.0e+00 0.0e+00 30 38  0  0  0  30 38  0  0  0   597
PCSetUpOnBlocks       15 1.0 1.2236e-04 1.0 1.29e+02 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     1
PCApply               15 1.0 4.1148e+00 1.0 1.02e+10 1.0 0.0e+00 0.0e+00 0.0e+00  8 42  0  0  0   8 42  0  0  0  2472
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container    32             16
              Matrix   144             96
      Matrix Coarsen     4              4
   Matrix Null Space     1              0
              Vector   316            221
           Index Set    81             78
   Star Forest Graph    66             49
       Krylov Solver    11              4
      Preconditioner    11              4
         PetscRandom     4              4
    Distributed Mesh     8              4
     Discrete System     8              4
           Weak Form     8              4
              Viewer     1              0
========================================================================================================================
Average time to get PetscTime(): 3.99e-08
#PETSc Option Table entries:
-ksp_monitor_true_residual # (source: command line)
-log_view # (source: command line)
-pc_gamg_verbose 2 # (source: code)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --with-debugging=0 --with-mpi=1 --with-ssl=0 --with-x=0 --with-64-bit-indices=0 --with-mumps-lib="-L/opt/aster/20240327/gcc13-openblas-ompi4/mumps-5.6.2/lib -lzmumps -ldmumps -lmumps_common -lpord -L/opt/aster/20240327/gcc13-openblas-ompi4/scotch-7.0.4/lib -lesmumps -lptscotch -lptscotcherr -lptscotcherrexit -lscotch -lscotcherr -lscotcherrexit -L/opt/aster/20240327/gcc13-openblas-ompi4/parmetis-4.0.3_aster3/lib -lparmetis" --with-mumps-include=/opt/aster/20240327/gcc13-openblas-ompi4/mumps-5.6.2/include --with-blaslapack-lib=-lopenblas --with-scalapack-lib="-L/opt/aster/20240327/gcc13-openblas-ompi4/scalapack-2.2.0/lib -lscalapack " --with-python=1 --with-petsc4py=1 --download-ml=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/pkg-trilinos-ml-v13.2.0.tar.gz --download-sowing=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/sowing_v1.1.26-p8.tar.gz --download-hypre=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/hypre_v2.29.0.tar.gz --download-superlu=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/SuperLU_v6.0.1.tar.gz --download-slepc=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/slepc-v3.20.1.tar.gz --download-slepc-configure-arguments="--with-slepc4py --download-arpack=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/arpack_3.9.0.tar.gz" --download-hpddm=/root/codeaster-prerequisites-20240327-oss/.build-gcc13-openblas-ompi4/content/3rd/hpddm_201eecd26177f88d7bb6287251877d8013fb64d2.tar.gz --with-openmp=1 --prefix=/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5 CC=mpicc CXX=mpicxx FC=mpif90 FCFLAGS=" -fallow-argument-mismatch" LIBS="-lgomp -lz"
-----------------------------------------
Libraries compiled on 2025-01-23 15:23:23 on buildkitsandbox 
Machine characteristics: Linux-5.10.230-223.885.amzn2.aarch64-aarch64-with-glibc2.40
Using PETSc directory: /opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5
Using PETSc arch: 
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  -fopenmp 
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O   -fopenmp   -fopenmp
-----------------------------------------

Using include paths: -I/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5/include -I/opt/aster/20240327/gcc13-openblas-ompi4/mumps-5.6.2/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5/lib -lpetsc -Wl,-rpath,/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/petsc-v3.20.5/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/mumps-5.6.2/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/scotch-7.0.4/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/parmetis-4.0.3_aster3/lib -L/opt/aster/20240327/gcc13-openblas-ompi4/scalapack-2.2.0/lib -Wl,-rpath,/usr/lib/aarch64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/aarch64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/aarch64-linux-gnu/13 -L/usr/lib/gcc/aarch64-linux-gnu/13 -Wl,-rpath,/usr/lib/aarch64-linux-gnu -L/usr/lib/aarch64-linux-gnu -Wl,-rpath,/lib/aarch64-linux-gnu -L/lib/aarch64-linux-gnu -lHYPRE -lzmumps -ldmumps -lmumps_common -lpord -lesmumps -lptscotch -lptscotcherr -lptscotcherrexit -lscotch -lscotcherr -lscotcherrexit -lparmetis -lscalapack -lsuperlu -lml -lopenblas -lm -lgomp -lz -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lgfortran -lm -lgfortran -lgcc_s -lstdc++ -lgomp -lz
-----------------------------------------

WARNING! There are options you set that were not used!
WARNING! could be spelling mistake, etc!
There is one unused database option. It is:
Option left: name:-pc_gamg_verbose value: 2 source: code

 ╔════════════════════════════════════════════════════════════════════════════════════════════════╗
 ║ <I> <CATAMESS_89>                                                                              ║
 ║                                                                                                ║
 ║ Liste des alarmes émises lors de l'exécution du calcul.                                        ║
 ║                                                                                                ║
 ║     Les alarmes que vous avez choisies d'ignorer sont précédées de (*).                        ║
 ║     Nombre d'occurrences pour chacune des alarmes :                                            ║
 ║            aucune alarme                                                                       ║
 ╚════════════════════════════════════════════════════════════════════════════════════════════════╝

<I> <FIN> ARRET NORMAL DANS "FIN" PAR APPEL A "JEFINI".
  
 <I> <FIN> MEMOIRE JEVEUX MINIMALE REQUISE POUR L'EXECUTION :                    1037.52 Mo
 <I> <FIN> MEMOIRE JEVEUX OPTIMALE REQUISE POUR L'EXECUTION :                    1724.47 Mo
 <I> <FIN> MAXIMUM DE MEMOIRE UTILISEE PAR LE PROCESSUS LORS DE L'EXECUTION :    4442.27 Mo
  
 <I>       FERMETURE DES BASES EFFECTUEE
  
   STATISTIQUES CONCERNANT L'ALLOCATION DYNAMIQUE :
     TAILLE CUMULEE MAXIMUM            :                 1724  Mo.
     TAILLE CUMULEE LIBEREE            :                 2135  Mo.
     NOMBRE TOTAL D'ALLOCATIONS        :             16739899
     NOMBRE TOTAL DE LIBERATIONS       :             16739231
     APPELS AU MECANISME DE LIBERATION :                    0
     TAILLE MEMOIRE CUMULEE RECUPEREE  :                    0  Mo.
     VOLUME DES LECTURES               :                    0  Mo.
     VOLUME DES ECRITURES              :                    0  Mo.
  
   MEMOIRE JEVEUX MINIMALE REQUISE POUR L'EXECUTION :    1037.52 Mo
     - IMPOSE DE NOMBREUX ACCES DISQUE
     - RALENTIT LA VITESSE D'EXECUTION
   MEMOIRE JEVEUX OPTIMALE REQUISE POUR L'EXECUTION :    1724.47 Mo
     - LIMITE LES ACCES DISQUE
     - AMELIORE LA VITESSE D'EXECUTION
   MAXIMUM DE MEMOIRE UTILISEE PAR LE PROCESSUS     :    4442.27 Mo
     - COMPREND LA MEMOIRE CONSOMMEE PAR  JEVEUX, 
       LE SUPERVISEUR PYTHON, LES LIBRAIRIES EXTERNES
  
 <I>       FIN D'EXECUTION LE : VE-24-JANV-2025 13:35:34
INFO './glob.1' deleted
Deleting './glob.2': No such file or directory
INFO './vola.1' deleted
Deleting './vola.2': No such file or directory

 ********************************************************************************
 * COMMAND                  :       USER :     SYSTEM :   USER+SYS :    ELAPSED *
 ********************************************************************************
 * DEBUT                    :       0.03 :       0.10 :       0.13 :       0.13 *
 * DEFI_FICHIER             :       0.00 :       0.00 :       0.00 :       0.00 *
 * DEFI_FICHIER             :       0.00 :       0.01 :       0.01 :       0.01 *
 * CREA_MAILLAGE            :       3.72 :       0.42 :       4.14 :       4.15 *
 * AFFE_MODELE              :       1.25 :       0.00 :       1.25 :       1.26 *
 * DEFI_MATERIAU            :       0.03 :       0.00 :       0.03 :       0.02 *
 * AFFE_MATERIAU            :       0.01 :       0.00 :       0.01 :       0.02 *
 * AFFE_CHAR_CINE           :       0.17 :       0.00 :       0.17 :       0.16 *
 * AFFE_CHAR_CINE           :       0.16 :       0.00 :       0.16 :       0.17 *
 * CALC_MATR_ELEM           :       2.98 :       0.04 :       3.02 :       3.02 *
 * NUME_DDL                 :       1.48 :       0.78 :       2.26 :       2.26 *
 * ASSE_MATRICE             :       1.21 :       0.06 :       1.27 :       1.28 *
 * CREA_CHAMP               :       0.04 :       0.01 :       0.05 :       0.05 *
 * CALC_CHAR_CINE           :       0.03 :       0.01 :       0.04 :       0.04 *
 * FIN                      :       0.02 :       0.00 :       0.02 :       0.03 *
 *  . check syntax          :       0.02 :       0.00 :       0.02 :       0.02 *
 *  . fortran               :       4.83 :       0.53 :       5.36 :       5.36 *
 *  . cleanup               :       0.08 :       0.00 :       0.08 :       0.11 *
 ********************************************************************************
 * TOTAL_JOB                :      48.88 :       4.22 :      53.10 :      53.10 *
 ********************************************************************************

# Mémoire (Mo) :  4442.27 /  4417.12 /  1724.47 /  1037.52 (VmPeak / VmSize / Optimum / Minimum)
# Fin commande #0015   user+syst:        0.02s (syst:        0.00s, elaps:        0.03s)
# ----------------------------------------------------------------------------------------------
End of the Code_Aster execution
Code_Aster MPI exits normally
Exited

EXECUTION_CODE_ASTER_EXIT_127=0


execution ended (command file #1): OK

# ------------------------------------------------------------------------------
Content of /tmp/run_aster_iwfw1npd/proc.0 after execution:
.:
total 64
-rw-r--r-- 1 aster aster   187 Jan 24 13:34 127.export
drwxr-xr-x 2 aster aster     6 Jan 24 13:34 REPE_IN
drwxr-xr-x 2 aster aster     6 Jan 24 13:34 REPE_OUT
-rw-r--r-- 1 aster aster 15367 Jan 24 13:35 asrun.log
-rw-r--r-- 1 aster aster 44603 Jan 24 13:35 fort.6
-rw-r--r-- 1 aster aster     0 Jan 24 13:34 fort.8
-rw-r--r-- 1 aster aster     0 Jan 24 13:34 fort.9

REPE_OUT:
total 0


# ------------------------------------------------------------------------------
Execution summary
                                      cpu     system    cpu+sys    elapsed
--------------------------------------------------------------------------------
Preparation of environment           0.00       0.00       0.00       0.00
Execution of code_aster             49.26       4.61      53.87      53.96
Copying results                      0.01       0.00       0.01       0.01
--------------------------------------------------------------------------------
Total                               49.27       4.61      53.88      53.97
--------------------------------------------------------------------------------

------------------------------------------------------------
--- DIAGNOSTIC JOB : OK
------------------------------------------------------------

